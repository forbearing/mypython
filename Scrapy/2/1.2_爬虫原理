1:爬虫基本流程
    1:发送请求
        通过 HTTP 库向目标站点发起请求，即发送一个 Request，请求可以包含额外的 headers
        等信息，等待服务器响应
    2:获取响应内容
        如果服务器能正常响应，会得到一个 Response，Response 的内容便是所要获取的页面
        内容，类型可能是 HTML、Json 字符串，二进制数据（如图片视频）等类型
    3:解析内容
        得到的内容可能是 HTML，可以用正则表达式，网页解析库进行解析。可能是 Json，
        可以直接转为 JSON 对象，可能是二进制数据，可以做保存或者进一步处理。
    4:保存数据
        保存形式多种多样，可以存为文本，也可以保存到数据库，或者保存成特定格式

2:能爬怎样的数据
    1:网页文本: 如 HTML 文档, Json 格式
    2:图片: 获取的是二进制文件,保存为图片格式
    3:视频: 同为二进制文件,保存为视频格式即可
    4:其他: 只要能看到的,都能获取

3:解析方式
    1:直接处理
    2:Json 解析
    3:正则表达式
    4:BeautifulSoup
    5:PyQuery
    6:XPath

4:怎样解决 JavaScript 渲染的问题
    1:分析 Ajax 请求
    2:Selenium/WebDriver(就像一个浏览器一样来加载一个网页)
        from selenium import webdriver
        driver = webdriver.Chrome()
        driver.get('https://www.zhihu.com') 
        print(driver.page_source)
    3:Splash
    4:PyV8, Ghost.py

5:怎样来保存数据
    1:纯文本, Json, Xml
    2:关系型数据库: MySQL, Oracle, SQL Server 等具有结构化表结构的形式存储
    3:非关系型数据库: MongoDB, Redis 等 key-value 形式存储
    4:二进制文件: 如图片,视频,音频等直接保存成特定格式



============================================================================


Python 技能
    1:Python 基础语法
    2:如何抓取页面
        http 请求处理, urllib 处理后的请求可以模拟浏览器发送请求,获取服务器响应的文件
    3:解析服务器响应的内容
        1:re, xpath, BeautifulSoup4, jsonpath, pyquery
        2:目的是使用某种描述性语言来提取匹配规则的数据
    4:如何采取动态 HTML, 验证码的处理
        1:通用的动态页面采集
            Selenium + PhantomJS(无界面浏览器),模拟真实的浏览器加载 JS, Ajax 
            等非静态页面数据
        2:Tesseract:
            机器学习库,机器图像识别系统(识别图片中的文本)
    5:Srapy 框架
        1:中国最常见的框架 Scrapy, PySpider
        2:高定制性高性能(异步网络框架twisted), 所以数据下载速度非常快, 提供了
          数据存储,数据下载,提起规则等组件
    6:分布式策略
        1:scrapy-redis
        2:在 Scrapy 的基础上添加了一套以 Redis 数据为核心的一套组件,让 Scrapy 框架
          支持分布式的功能, 主要在 Redis 里做请求指纹去重,请求分配,数据临时存储
    7:反爬虫技术
        User-Agent
        代理
        验证码
        动态数据加载
        加密数据
    8:是否要反爬虫
        机器成本+人力成本>数据价值, 就不反了,一般做到封 IP 就可以了
    
网络爬虫类型
    通用网路爬虫
    增量式网络爬虫
    深层网络爬虫

通用网络爬虫
    1:概念: 搜索引擎用的爬虫系统
    2:用户群体: 搜索引擎
    3:目标: 尽可能把互联网上的所有网页下载下来,放到本地服务器形成备份. 再对这些网页
      做相关处理(提取关键字,去掉广告等), 最后提供一个用户检测接口
    4:爬取流程
        1:首先选取一部分已有的 URL, 把这些 URL 放到带爬队列
        2:从队列里提取出这些 URL, 然后解析 DNS 得到主机 IP. 然后去这个 IP 对应的服务器
          下载 HTML 页面, 保存到搜索引擎的本地服务器里, 之后把爬过的 URL 放入已爬取队列
        3:分析这些网页,找出网页里的 URL 路径,继续执行第二步,知道爬取条件结束
    5:搜索引擎如何知道一个新网站 URL
        1:主动向搜索引擎提交网址(百度站长平台)
        2:在其他网站里设置网站的外链接
        3:搜索引擎会和 DNS 服务商合作,可以快速收录新的网站
    6:通用爬虫工作流程
        1:爬取网页
        2:存储数据
        3:内容处理
        4:提供检索/排名服务
    7:
